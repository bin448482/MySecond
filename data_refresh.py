#!/usr/bin/env python3
"""
æ•°æ®åˆ·æ–°å·¥å…·
æ ¹æ®data_completeness_checker.pyç”Ÿæˆçš„æŠ¥å‘Šï¼Œæœ‰é’ˆå¯¹æ€§åœ°åˆ·æ–°ç¼ºå¤±60å¤©æ•°æ®çš„è‚¡ç¥¨

ä½¿ç”¨æ–¹æ³•:
  python data_refresh.py smart-refresh [--test-mode] [--max-stocks N] [--yes]
  python data_refresh.py full-refresh [--test-mode] [--max-stocks N] [--yes]
  python data_refresh.py cleanup [--progress-file PATH] [--yes]
  python data_refresh.py check [--target-days N] [--output-file PATH]
"""

import sqlite3
import pandas as pd
from datetime import datetime, timedelta, date
import logging
import os
import sys
import time
import json
import argparse
from collections import defaultdict

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from database import DatabaseManager
from enhanced_data_fetcher import EnhancedDataFetcher

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class DataRefreshManager:
    """æ•°æ®åˆ·æ–°ç®¡ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç®¡ç†å™¨"""
        self.db = DatabaseManager()
        self.data_fetcher = EnhancedDataFetcher(self.db)
        self.issues = {
            'missing_days': {},      # ç¼ºå¤±å¤©æ•°çš„è‚¡ç¥¨
            'duplicate_days': {},    # é‡å¤å¤©æ•°çš„è‚¡ç¥¨
            'incomplete_stocks': [], # æ•°æ®ä¸å®Œæ•´çš„è‚¡ç¥¨
            'complete_stocks': []    # æ•°æ®å®Œæ•´çš„è‚¡ç¥¨
        }
    
    def backup_database(self) -> str:
        """
        å¤‡ä»½æ•°æ®åº“
        
        Returns:
            å¤‡ä»½æ–‡ä»¶è·¯å¾„
        """
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_path = f"data/backup_before_refresh_{timestamp}.db"
        
        try:
            import shutil
            shutil.copy2(self.db.db_path, backup_path)
            logger.info(f"æ•°æ®åº“å·²å¤‡ä»½è‡³: {backup_path}")
            return backup_path
        except Exception as e:
            logger.error(f"æ•°æ®åº“å¤‡ä»½å¤±è´¥: {e}")
            return None
    
    def get_incomplete_stocks_from_report(self, report_file: str = "data/completeness_report.json") -> dict:
        """
        ä»å®Œæ•´æ€§æŠ¥å‘Šä¸­è·å–éœ€è¦åˆ·æ–°çš„è‚¡ç¥¨
        
        Args:
            report_file: æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
            
        Returns:
            åŒ…å«éœ€è¦åˆ·æ–°è‚¡ç¥¨ä¿¡æ¯çš„å­—å…¸
        """
        if not os.path.exists(report_file):
            logger.warning(f"æŠ¥å‘Šæ–‡ä»¶ä¸å­˜åœ¨: {report_file}")
            return {}
        
        try:
            with open(report_file, 'r', encoding='utf-8') as f:
                report = json.load(f)
            
            # æå–éœ€è¦åˆ·æ–°çš„è‚¡ç¥¨
            incomplete_stocks = {
                'missing_data': [],
                'duplicate_data': [],
                'missing_and_duplicate': []
            }
            
            detailed_results = report.get('detailed_results', {})
            
            for symbol, stock_data in detailed_results.items():
                status = stock_data.get('status', '')
                if status in ['missing_data', 'duplicate_data', 'missing_and_duplicate']:
                    incomplete_stocks[status].append({
                        'symbol': symbol,
                        'missing_days': len(stock_data.get('missing_days', [])),
                        'duplicate_days': len(stock_data.get('duplicate_days', [])),
                        'completeness_rate': stock_data.get('completeness_rate', 0)
                    })
            
            total_incomplete = sum(len(stocks) for stocks in incomplete_stocks.values())
            logger.info(f"ä»æŠ¥å‘Šä¸­æ‰¾åˆ° {total_incomplete} åªéœ€è¦åˆ·æ–°çš„è‚¡ç¥¨")
            logger.info(f"  ç¼ºå¤±æ•°æ®: {len(incomplete_stocks['missing_data'])} åª")
            logger.info(f"  é‡å¤æ•°æ®: {len(incomplete_stocks['duplicate_data'])} åª")
            logger.info(f"  ç¼ºå¤±+é‡å¤: {len(incomplete_stocks['missing_and_duplicate'])} åª")
            
            return incomplete_stocks
            
        except Exception as e:
            logger.error(f"è¯»å–æŠ¥å‘Šæ–‡ä»¶å¤±è´¥: {e}")
            return {}
    
    def get_stock_symbols(self) -> list:
        """
        è·å–æ‰€æœ‰è‚¡ç¥¨ä»£ç 
        
        Returns:
            è‚¡ç¥¨ä»£ç åˆ—è¡¨
        """
        conn = self.db.get_connection()
        try:
            # ä»stock_infoè¡¨è·å–è‚¡ç¥¨ä»£ç 
            query = "SELECT DISTINCT symbol FROM stock_info ORDER BY symbol"
            df = pd.read_sql_query(query, conn)
            symbols = df['symbol'].tolist()
            logger.info(f"è·å–åˆ° {len(symbols)} åªè‚¡ç¥¨ä»£ç ")
            return symbols
        except Exception as e:
            logger.error(f"è·å–è‚¡ç¥¨ä»£ç å¤±è´¥: {e}")
            return []
        finally:
            conn.close()
    
    def clear_stock_data(self, symbols: list) -> bool:
        """
        æ¸…é™¤æŒ‡å®šè‚¡ç¥¨çš„æ•°æ®
        
        Args:
            symbols: è¦æ¸…é™¤çš„è‚¡ç¥¨ä»£ç åˆ—è¡¨
            
        Returns:
            æ¸…é™¤æ˜¯å¦æˆåŠŸ
        """
        if not symbols:
            return True
            
        conn = self.db.get_connection()
        cursor = conn.cursor()
        
        try:
            # è·å–æ¸…é™¤å‰çš„è®°å½•æ•°
            placeholders = ','.join(['?' for _ in symbols])
            cursor.execute(f"SELECT COUNT(*) FROM daily_data WHERE symbol IN ({placeholders})", symbols)
            before_count = cursor.fetchone()[0]
            
            # æ¸…é™¤æŒ‡å®šè‚¡ç¥¨çš„æ—¥çº¿æ•°æ®
            cursor.execute(f"DELETE FROM daily_data WHERE symbol IN ({placeholders})", symbols)
            
            # æ¸…é™¤æŒ‡å®šè‚¡ç¥¨çš„æŠ€æœ¯æŒ‡æ ‡æ•°æ®
            cursor.execute(f"DELETE FROM technical_indicators WHERE symbol IN ({placeholders})", symbols)
            
            conn.commit()
            
            logger.info(f"å·²æ¸…é™¤ {len(symbols)} åªè‚¡ç¥¨çš„ {before_count} æ¡æ•°æ®è®°å½•")
            return True
            
        except Exception as e:
            logger.error(f"æ¸…é™¤è‚¡ç¥¨æ•°æ®å¤±è´¥: {e}")
            conn.rollback()
            return False
        finally:
            conn.close()
    
    def clear_all_daily_data(self) -> bool:
        """
        æ¸…é™¤æ‰€æœ‰æ—¥çº¿æ•°æ®è¡¨
        
        Returns:
            æ¸…é™¤æ˜¯å¦æˆåŠŸ
        """
        conn = self.db.get_connection()
        cursor = conn.cursor()
        
        try:
            # è·å–æ¸…é™¤å‰çš„è®°å½•æ•°
            cursor.execute("SELECT COUNT(*) FROM daily_data")
            before_count = cursor.fetchone()[0]
            
            # æ¸…é™¤æ—¥çº¿æ•°æ®
            cursor.execute("DELETE FROM daily_data")
            
            # æ¸…é™¤æŠ€æœ¯æŒ‡æ ‡æ•°æ®ï¼ˆä¾èµ–äºæ—¥çº¿æ•°æ®ï¼‰
            cursor.execute("DELETE FROM technical_indicators")
            
            conn.commit()
            
            logger.info(f"å·²æ¸…é™¤ {before_count} æ¡æ—¥çº¿æ•°æ®è®°å½•")
            logger.info("å·²æ¸…é™¤æ‰€æœ‰æŠ€æœ¯æŒ‡æ ‡æ•°æ®")
            return True
            
        except Exception as e:
            logger.error(f"æ¸…é™¤æ•°æ®å¤±è´¥: {e}")
            conn.rollback()
            return False
        finally:
            conn.close()
    
    def refresh_stock_data(self, symbols: list, days: int = 60, max_stocks: int = None) -> dict:
        """
        é‡æ–°è·å–è‚¡ç¥¨æ•°æ®
        
        Args:
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            days: è·å–å¤©æ•°
            max_stocks: æœ€å¤§å¤„ç†è‚¡ç¥¨æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰
            
        Returns:
            åˆ·æ–°ç»“æœç»Ÿè®¡
        """
        if max_stocks:
            symbols = symbols[:max_stocks]
            logger.info(f"é™åˆ¶å¤„ç†è‚¡ç¥¨æ•°é‡ä¸º: {max_stocks}")
        
        total_symbols = len(symbols)
        success_count = 0
        failed_count = 0
        total_records = 0
        
        logger.info(f"å¼€å§‹é‡æ–°è·å– {total_symbols} åªè‚¡ç¥¨çš„æ•°æ®...")
        
        for i, symbol in enumerate(symbols, 1):
            try:
                logger.info(f"[{i}/{total_symbols}] è·å– {symbol} çš„æ•°æ®...")
                
                # è·å–è‚¡ç¥¨æ•°æ® - ä½¿ç”¨å›ºå®šå»¶è¿Ÿç®¡ç†å™¨
                records_added = self.data_fetcher.update_stock_data_with_fixed_delay(symbol, days=days)
                
                if records_added > 0:
                    success_count += 1
                    total_records += records_added
                    logger.info(f"  âœ… {symbol}: æˆåŠŸè·å– {records_added} æ¡è®°å½•")
                else:
                    failed_count += 1
                    logger.warning(f"  âŒ {symbol}: è·å–å¤±è´¥æˆ–æ— æ–°æ•°æ®")
                
                # æ¯å¤„ç†10åªè‚¡ç¥¨æ˜¾ç¤ºè¿›åº¦
                if i % 10 == 0:
                    logger.info(f"è¿›åº¦: {i}/{total_symbols} ({i/total_symbols*100:.1f}%), æˆåŠŸ: {success_count}, å¤±è´¥: {failed_count}")
                
                # enhanced_data_fetcherå·²ç»å¤„ç†äº†å»¶è¿Ÿç®¡ç†ï¼Œæ— éœ€é¢å¤–å»¶è¿Ÿ
                # ä½†å¯ä»¥æ£€æŸ¥ç½‘ç»œçŠ¶æ€ï¼Œå†³å®šæ˜¯å¦éœ€è¦æš‚åœ
                if self.data_fetcher.delay_manager.should_pause():
                    logger.warning(f"ç½‘ç»œçŠ¶æ€è¿‡å·®ï¼Œæš‚åœå¤„ç†ã€‚å·²å¤„ç† {i}/{total_symbols}")
                    break
                
            except Exception as e:
                failed_count += 1
                logger.error(f"  âŒ {symbol}: è·å–å‡ºé”™ - {e}")
        
        results = {
            'total_symbols': total_symbols,
            'success_count': success_count,
            'failed_count': failed_count,
            'total_records': total_records,
            'success_rate': success_count / total_symbols * 100 if total_symbols > 0 else 0
        }
        
        # è·å–ç½‘ç»œçŠ¶æ€å’ŒæŒ‡æ ‡æ‘˜è¦
        metrics_summary = self.data_fetcher.get_metrics_summary()
        
        logger.info(f"æ•°æ®åˆ·æ–°å®Œæˆ:")
        logger.info(f"  æ€»è‚¡ç¥¨æ•°: {total_symbols}")
        logger.info(f"  æˆåŠŸ: {success_count} ({results['success_rate']:.1f}%)")
        logger.info(f"  å¤±è´¥: {failed_count}")
        logger.info(f"  æ€»è®°å½•æ•°: {total_records}")
        logger.info(f"  ç½‘ç»œçŠ¶æ€: {metrics_summary['network_status']}")
        logger.info(f"  è¯·æ±‚æˆåŠŸç‡: {metrics_summary['success_rate']}")
        logger.info(f"  å¹³å‡å“åº”æ—¶é—´: {metrics_summary['average_response_time']}")
        
        return results
    
    def get_trading_days(self, start_date: date, end_date: date) -> set:
        """
        è·å–äº¤æ˜“æ—¥é›†åˆï¼ˆæ’é™¤å‘¨æœ«ï¼‰
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            
        Returns:
            äº¤æ˜“æ—¥æœŸé›†åˆ
        """
        trading_days = set()
        current_date = start_date
        
        while current_date <= end_date:
            # æ’é™¤å‘¨æœ«ï¼ˆå‘¨å…­=5, å‘¨æ—¥=6ï¼‰
            if current_date.weekday() < 5:
                trading_days.add(current_date)
            current_date += timedelta(days=1)
        
        return trading_days
    
    def check_stock_completeness(self, symbol: str, target_days: int = 60) -> dict:
        """
        æ£€æŸ¥å•åªè‚¡ç¥¨çš„æ•°æ®å®Œå¤‡æ€§
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            target_days: ç›®æ ‡å¤©æ•°
            
        Returns:
            æ£€æŸ¥ç»“æœå­—å…¸
        """
        conn = self.db.get_connection()
        
        try:
            # è·å–è‚¡ç¥¨çš„æ‰€æœ‰æ•°æ®æ—¥æœŸ
            query = """
                SELECT date, COUNT(*) as count
                FROM daily_data 
                WHERE symbol = ? 
                GROUP BY date
                ORDER BY date DESC
            """
            
            df = pd.read_sql_query(query, conn, params=(symbol,))
            
            if df.empty:
                return {
                    'symbol': symbol,
                    'status': 'no_data',
                    'total_records': 0,
                    'missing_days': [],
                    'duplicate_days': [],
                    'data_range': None
                }
            
            # è½¬æ¢æ—¥æœŸ
            df['date'] = pd.to_datetime(df['date']).dt.date
            actual_dates = set(df['date'].tolist())
            
            # æ‰¾å‡ºé‡å¤çš„æ—¥æœŸ
            duplicate_dates = df[df['count'] > 1]['date'].tolist()
            
            # è®¡ç®—ç›®æ ‡æ—¥æœŸèŒƒå›´ï¼ˆæœ€è¿‘60ä¸ªäº¤æ˜“æ—¥ï¼Œæ’é™¤ä»Šå¤©å› ä¸ºæ•°æ®è¦ä¸‹åˆ4ç‚¹åæ‰æ›´æ–°ï¼‰
            end_date = datetime.now().date() - timedelta(days=1)  # ä»æ˜¨å¤©å¼€å§‹è®¡ç®—
            start_date = end_date - timedelta(days=target_days + 20)  # å¤šåŠ 20å¤©ä»¥ç¡®ä¿è¦†ç›–60ä¸ªäº¤æ˜“æ—¥
            
            # è·å–ç†è®ºäº¤æ˜“æ—¥
            expected_trading_days = self.get_trading_days(start_date, end_date)
            
            # åªå–æœ€è¿‘çš„60ä¸ªäº¤æ˜“æ—¥
            expected_trading_days = sorted(expected_trading_days, reverse=True)[:target_days]
            expected_trading_days_set = set(expected_trading_days)
            
            # æ‰¾å‡ºç¼ºå¤±çš„æ—¥æœŸ
            missing_dates = expected_trading_days_set - actual_dates
            
            # è®¡ç®—å®é™…æ‹¥æœ‰çš„ç›®æ ‡æœŸé—´å†…çš„æ•°æ®
            actual_target_dates = actual_dates & expected_trading_days_set
            
            result = {
                'symbol': symbol,
                'total_records': len(df),
                'target_period_records': len(actual_target_dates),
                'expected_records': len(expected_trading_days_set),
                'missing_days': sorted(missing_dates, reverse=True),
                'duplicate_days': duplicate_dates,
                'data_range': {
                    'start': df['date'].min().isoformat() if not df.empty else None,
                    'end': df['date'].max().isoformat() if not df.empty else None
                },
                'completeness_rate': len(actual_target_dates) / len(expected_trading_days_set) * 100 if expected_trading_days_set else 0
            }
            
            # åˆ¤æ–­çŠ¶æ€
            if len(missing_dates) == 0 and len(duplicate_dates) == 0:
                result['status'] = 'complete'
            elif len(missing_dates) > 0 and len(duplicate_dates) > 0:
                result['status'] = 'missing_and_duplicate'
            elif len(missing_dates) > 0:
                result['status'] = 'missing_data'
            elif len(duplicate_dates) > 0:
                result['status'] = 'duplicate_data'
            else:
                result['status'] = 'unknown'
            
            return result
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥è‚¡ç¥¨ {symbol} å®Œå¤‡æ€§å¤±è´¥: {e}")
            return {
                'symbol': symbol,
                'status': 'error',
                'error': str(e)
            }
        finally:
            conn.close()
    
    def check_all_stocks_completeness(self, target_days: int = 60) -> dict:
        """
        æ£€æŸ¥æ‰€æœ‰è‚¡ç¥¨çš„æ•°æ®å®Œå¤‡æ€§
        
        Args:
            target_days: ç›®æ ‡å¤©æ•°
            
        Returns:
            å®Œæ•´çš„æ£€æŸ¥ç»“æœ
        """
        logger.info(f"å¼€å§‹æ£€æŸ¥æ‰€æœ‰è‚¡ç¥¨çš„æ•°æ®å®Œå¤‡æ€§ï¼ˆç›®æ ‡å¤©æ•°: {target_days}ï¼‰...")
        
        conn = self.db.get_connection()
        
        try:
            # è·å–æ‰€æœ‰æœ‰æ•°æ®çš„è‚¡ç¥¨
            stocks_query = """
                SELECT DISTINCT symbol 
                FROM daily_data 
                ORDER BY symbol
            """
            
            stocks_df = pd.read_sql_query(stocks_query, conn)
            total_stocks = len(stocks_df)
            
            logger.info(f"æ‰¾åˆ° {total_stocks} åªæœ‰æ•°æ®çš„è‚¡ç¥¨")
            
            # é‡ç½®é—®é¢˜ç»Ÿè®¡
            self.issues = {
                'missing_days': {},
                'duplicate_days': {},
                'incomplete_stocks': [],
                'complete_stocks': []
            }
            
            # ç»Ÿè®¡ç»“æœ
            results = {
                'check_time': datetime.now().isoformat(),
                'target_days': target_days,
                'total_stocks': total_stocks,
                'summary': {
                    'complete': 0,
                    'missing_data': 0,
                    'duplicate_data': 0,
                    'missing_and_duplicate': 0,
                    'no_data': 0,
                    'error': 0
                },
                'stocks': {}
            }
            
            # é€ä¸ªæ£€æŸ¥è‚¡ç¥¨
            for i, row in stocks_df.iterrows():
                symbol = row['symbol']
                
                if (i + 1) % 100 == 0:
                    logger.info(f"å·²æ£€æŸ¥ {i + 1}/{total_stocks} åªè‚¡ç¥¨...")
                
                stock_result = self.check_stock_completeness(symbol, target_days)
                results['stocks'][symbol] = stock_result
                
                # æ›´æ–°ç»Ÿè®¡
                status = stock_result.get('status', 'error')
                if status in results['summary']:
                    results['summary'][status] += 1
                
                # è®°å½•é—®é¢˜è‚¡ç¥¨
                if status == 'missing_data' or status == 'missing_and_duplicate':
                    if stock_result['missing_days']:
                        self.issues['missing_days'][symbol] = stock_result['missing_days']
                
                if status == 'duplicate_data' or status == 'missing_and_duplicate':
                    if stock_result['duplicate_days']:
                        self.issues['duplicate_days'][symbol] = stock_result['duplicate_days']
                
                if status == 'complete':
                    self.issues['complete_stocks'].append(symbol)
                else:
                    self.issues['incomplete_stocks'].append(symbol)
            
            logger.info("æ•°æ®å®Œå¤‡æ€§æ£€æŸ¥å®Œæˆ")
            return results
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥æ‰€æœ‰è‚¡ç¥¨å®Œå¤‡æ€§å¤±è´¥: {e}")
            return {}
        finally:
            conn.close()
    
    def generate_completeness_report(self, results: dict, output_file: str = "data/completeness_report.json") -> dict:
        """
        ç”Ÿæˆå®Œå¤‡æ€§æŠ¥å‘Š
        
        Args:
            results: æ£€æŸ¥ç»“æœ
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            æŠ¥å‘Šå­—å…¸
        """
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        report = {
            'metadata': {
                'check_time': results.get('check_time'),
                'target_days': results.get('target_days'),
                'total_stocks': results.get('total_stocks')
            },
            'summary': results.get('summary', {}),
            'issues': self.issues,
            'detailed_results': results.get('stocks', {})
        }
        
        # ä¿å­˜æŠ¥å‘Š
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"å®Œå¤‡æ€§æŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")
        
        # æ‰“å°æ‘˜è¦
        self.print_completeness_summary(results)
        
        return report
    
    def print_completeness_summary(self, results: dict):
        """æ‰“å°å®Œæ•´æ€§æ£€æŸ¥æ‘˜è¦"""
        print("\n" + "=" * 80)
        print("æ•°æ®å®Œå¤‡æ€§æ£€æŸ¥æŠ¥å‘Š")
        print("=" * 80)
        
        summary = results.get('summary', {})
        total = results.get('total_stocks', 0)
        
        print(f"æ£€æŸ¥æ—¶é—´: {results.get('check_time', 'N/A')}")
        print(f"ç›®æ ‡å¤©æ•°: {results.get('target_days', 60)} å¤©")
        print(f"æ€»è‚¡ç¥¨æ•°: {total:,} åª")
        
        print(f"\næ£€æŸ¥ç»“æœç»Ÿè®¡:")
        print("-" * 40)
        if total > 0:
            print(f"âœ… æ•°æ®å®Œæ•´: {summary.get('complete', 0):,} åª ({summary.get('complete', 0)/total*100:.1f}%)")
            print(f"âŒ ç¼ºå¤±æ•°æ®: {summary.get('missing_data', 0):,} åª ({summary.get('missing_data', 0)/total*100:.1f}%)")
            print(f"ğŸ”„ é‡å¤æ•°æ®: {summary.get('duplicate_data', 0):,} åª ({summary.get('duplicate_data', 0)/total*100:.1f}%)")
            print(f"âš ï¸  ç¼ºå¤±+é‡å¤: {summary.get('missing_and_duplicate', 0):,} åª ({summary.get('missing_and_duplicate', 0)/total*100:.1f}%)")
            print(f"â“ æ— æ•°æ®: {summary.get('no_data', 0):,} åª ({summary.get('no_data', 0)/total*100:.1f}%)")
            print(f"ğŸ’¥ æ£€æŸ¥é”™è¯¯: {summary.get('error', 0):,} åª ({summary.get('error', 0)/total*100:.1f}%)")
        else:
            print("âŒ æ•°æ®åº“ä¸­æ²¡æœ‰ä»»ä½•è‚¡ç¥¨æ•°æ®")
            print("ğŸ’¡ è¯·å…ˆè¿è¡Œæ•°æ®è·å–ç¨‹åºæ¥å¡«å……æ•°æ®")
        
        # æ˜¾ç¤ºé—®é¢˜è¯¦æƒ…
        if self.issues['missing_days']:
            print(f"\nç¼ºå¤±æ•°æ®çš„è‚¡ç¥¨ç¤ºä¾‹ (å‰10åª):")
            print("-" * 40)
            count = 0
            for symbol, missing_days in self.issues['missing_days'].items():
                if count >= 10:
                    break
                print(f"{symbol}: ç¼ºå¤± {len(missing_days)} å¤©")
                count += 1
            
            if len(self.issues['missing_days']) > 10:
                print(f"... è¿˜æœ‰ {len(self.issues['missing_days']) - 10} åªè‚¡ç¥¨æœ‰ç¼ºå¤±æ•°æ®")
        
        if self.issues['duplicate_days']:
            print(f"\né‡å¤æ•°æ®çš„è‚¡ç¥¨ç¤ºä¾‹ (å‰10åª):")
            print("-" * 40)
            count = 0
            for symbol, duplicate_days in self.issues['duplicate_days'].items():
                if count >= 10:
                    break
                print(f"{symbol}: é‡å¤ {len(duplicate_days)} å¤©")
                count += 1
            
            if len(self.issues['duplicate_days']) > 10:
                print(f"... è¿˜æœ‰ {len(self.issues['duplicate_days']) - 10} åªè‚¡ç¥¨æœ‰é‡å¤æ•°æ®")
        
        print("\n" + "=" * 80)
    
    def check_and_cleanup_failed_symbols(self, progress_file: str = "data/enhanced_batch_progress.json") -> dict:
        """
        æ£€æŸ¥å¤±è´¥è‚¡ç¥¨çš„æ•°æ®å®Œæ•´æ€§ï¼Œå¦‚æœæ•°æ®å®Œæ•´åˆ™ä»å¤±è´¥åˆ—è¡¨ä¸­æ¸…é™¤
        
        Args:
            progress_file: æ‰¹å¤„ç†è¿›åº¦æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ¸…ç†ç»“æœç»Ÿè®¡
        """
        if not os.path.exists(progress_file):
            logger.warning(f"è¿›åº¦æ–‡ä»¶ä¸å­˜åœ¨: {progress_file}")
            return {}
        
        try:
            # è¯»å–è¿›åº¦æ–‡ä»¶
            with open(progress_file, 'r', encoding='utf-8') as f:
                progress_data = json.load(f)
            
            failed_symbols = progress_data.get('failed_symbols', [])
            if not failed_symbols:
                logger.info("æ²¡æœ‰å¤±è´¥çš„è‚¡ç¥¨éœ€è¦æ£€æŸ¥")
                return {'total_failed': 0, 'checked': 0, 'cleaned': 0, 'still_failed': 0}
            
            logger.info(f"å¼€å§‹æ£€æŸ¥ {len(failed_symbols)} åªå¤±è´¥è‚¡ç¥¨çš„æ•°æ®å®Œæ•´æ€§...")
            
            # æ£€æŸ¥æ¯åªè‚¡ç¥¨çš„æ•°æ®å®Œæ•´æ€§
            cleaned_symbols = []
            still_failed_symbols = []
            checked_count = 0
            
            for symbol in failed_symbols:
                try:
                    checked_count += 1
                    
                    # æ£€æŸ¥è‚¡ç¥¨æ•°æ®å®Œæ•´æ€§
                    is_complete = self._check_stock_data_completeness(symbol)
                    
                    if is_complete:
                        cleaned_symbols.append(symbol)
                        logger.info(f"âœ… {symbol}: æ•°æ®å®Œæ•´ï¼Œä»å¤±è´¥åˆ—è¡¨ä¸­æ¸…é™¤")
                    else:
                        still_failed_symbols.append(symbol)
                        logger.debug(f"âŒ {symbol}: æ•°æ®ä»ä¸å®Œæ•´")
                    
                    # æ¯æ£€æŸ¥50åªè‚¡ç¥¨æ˜¾ç¤ºè¿›åº¦
                    if checked_count % 50 == 0:
                        logger.info(f"è¿›åº¦: {checked_count}/{len(failed_symbols)}, å·²æ¸…ç†: {len(cleaned_symbols)}")
                        
                except Exception as e:
                    logger.error(f"æ£€æŸ¥è‚¡ç¥¨ {symbol} æ—¶å‡ºé”™: {e}")
                    still_failed_symbols.append(symbol)
            
            # æ›´æ–°è¿›åº¦æ–‡ä»¶
            if cleaned_symbols:
                progress_data['failed_symbols'] = still_failed_symbols
                progress_data['failed_count'] = len(still_failed_symbols)
                progress_data['last_cleanup'] = datetime.now().isoformat()
                
                # å¤‡ä»½åŸæ–‡ä»¶
                backup_file = f"{progress_file}.backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                with open(backup_file, 'w', encoding='utf-8') as f:
                    json.dump(progress_data, f, indent=2, ensure_ascii=False)
                logger.info(f"åŸè¿›åº¦æ–‡ä»¶å·²å¤‡ä»½åˆ°: {backup_file}")
                
                # å†™å…¥æ›´æ–°åçš„æ–‡ä»¶
                with open(progress_file, 'w', encoding='utf-8') as f:
                    json.dump(progress_data, f, indent=2, ensure_ascii=False)
                
                logger.info(f"è¿›åº¦æ–‡ä»¶å·²æ›´æ–°ï¼Œæ¸…é™¤äº† {len(cleaned_symbols)} åªè‚¡ç¥¨")
            
            results = {
                'total_failed': len(failed_symbols),
                'checked': checked_count,
                'cleaned': len(cleaned_symbols),
                'still_failed': len(still_failed_symbols),
                'cleaned_symbols': cleaned_symbols,
                'still_failed_symbols': still_failed_symbols
            }
            
            logger.info("å¤±è´¥è‚¡ç¥¨æ•°æ®å®Œæ•´æ€§æ£€æŸ¥å®Œæˆ:")
            logger.info(f"  æ€»å¤±è´¥è‚¡ç¥¨: {results['total_failed']} åª")
            logger.info(f"  å·²æ£€æŸ¥: {results['checked']} åª")
            logger.info(f"  æ•°æ®å®Œæ•´(å·²æ¸…ç†): {results['cleaned']} åª")
            logger.info(f"  ä»ç„¶å¤±è´¥: {results['still_failed']} åª")
            
            return results
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥å¤±è´¥è‚¡ç¥¨æ•°æ®å®Œæ•´æ€§æ—¶å‡ºé”™: {e}")
            return {}
    
    def _check_stock_data_completeness(self, symbol: str, days: int = 60) -> bool:
        """
        æ£€æŸ¥å•åªè‚¡ç¥¨çš„æ•°æ®å®Œæ•´æ€§ - ä½¿ç”¨ä¸data_completeness_checker.pyç›¸åŒçš„ä¸¥æ ¼æ ‡å‡†
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            days: æ£€æŸ¥å¤©æ•°
            
        Returns:
            æ•°æ®æ˜¯å¦å®Œæ•´ï¼ˆå¿…é¡»æ»¡è¶³ï¼šæ— ç¼ºå¤±å¤©æ•° AND æ— é‡å¤å¤©æ•°ï¼‰
        """
        try:
            conn = self.db.get_connection()
            
            # è·å–è‚¡ç¥¨çš„æ‰€æœ‰æ•°æ®æ—¥æœŸå’Œè®¡æ•°
            query = """
                SELECT date, COUNT(*) as count
                FROM daily_data
                WHERE symbol = ?
                GROUP BY date
                ORDER BY date DESC
            """
            
            import pandas as pd
            from datetime import date, timedelta
            
            df = pd.read_sql_query(query, conn, params=(symbol,))
            conn.close()
            
            if df.empty:
                logger.debug(f"è‚¡ç¥¨ {symbol} æ²¡æœ‰å†å²æ•°æ®")
                return False
            
            # è½¬æ¢æ—¥æœŸ
            df['date'] = pd.to_datetime(df['date']).dt.date
            actual_dates = set(df['date'].tolist())
            
            # æ‰¾å‡ºé‡å¤çš„æ—¥æœŸ
            duplicate_dates = df[df['count'] > 1]['date'].tolist()
            if duplicate_dates:
                logger.debug(f"è‚¡ç¥¨ {symbol} æœ‰é‡å¤æ•°æ®: {len(duplicate_dates)} å¤©")
                return False
            
            # è®¡ç®—ç›®æ ‡æ—¥æœŸèŒƒå›´ï¼ˆæœ€è¿‘60ä¸ªäº¤æ˜“æ—¥ï¼Œæ’é™¤ä»Šå¤©å› ä¸ºæ•°æ®è¦ä¸‹åˆ4ç‚¹åæ‰æ›´æ–°ï¼‰
            end_date = datetime.now().date() - timedelta(days=1)  # ä»æ˜¨å¤©å¼€å§‹è®¡ç®—
            start_date = end_date - timedelta(days=days + 20)  # å¤šåŠ 20å¤©ä»¥ç¡®ä¿è¦†ç›–60ä¸ªäº¤æ˜“æ—¥
            
            # è·å–ç†è®ºäº¤æ˜“æ—¥ï¼ˆæ’é™¤å‘¨æœ«ï¼‰
            expected_trading_days = set()
            current_date = start_date
            
            while current_date <= end_date:
                # æ’é™¤å‘¨æœ«ï¼ˆå‘¨å…­=5, å‘¨æ—¥=6ï¼‰
                if current_date.weekday() < 5:
                    expected_trading_days.add(current_date)
                current_date += timedelta(days=1)
            
            # åªå–æœ€è¿‘çš„60ä¸ªäº¤æ˜“æ—¥
            expected_trading_days = sorted(expected_trading_days, reverse=True)[:days]
            expected_trading_days_set = set(expected_trading_days)
            
            # æ‰¾å‡ºç¼ºå¤±çš„æ—¥æœŸ
            missing_dates = expected_trading_days_set - actual_dates
            
            if missing_dates:
                logger.debug(f"è‚¡ç¥¨ {symbol} ç¼ºå¤±æ•°æ®: {len(missing_dates)} å¤©")
                return False
            
            # åªæœ‰åŒæ—¶æ»¡è¶³ï¼šæ— ç¼ºå¤±æ•°æ® AND æ— é‡å¤æ•°æ®ï¼Œæ‰ç®—å®Œæ•´
            logger.debug(f"è‚¡ç¥¨ {symbol} æ•°æ®å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡: æ— ç¼ºå¤±æ— é‡å¤")
            return True
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥è‚¡ç¥¨ {symbol} æ•°æ®å®Œæ•´æ€§æ—¶å‡ºé”™: {e}")
            return False


def cmd_smart_refresh(args, manager):
    """æ‰§è¡Œæ™ºèƒ½åˆ·æ–°å‘½ä»¤"""
    print("\n" + "=" * 60)
    print("æ™ºèƒ½åˆ·æ–°æ¨¡å¼")
    print("=" * 60)
    
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æŠ¥å‘Šæ–‡ä»¶
    report_file = args.report_file or "data/completeness_report.json"
    if not os.path.exists(report_file):
        print(f"âŒ æœªæ‰¾åˆ°å®Œæ•´æ€§æŠ¥å‘Šæ–‡ä»¶: {report_file}")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œ: python data_refresh.py check")
        return False
    
    # ä»æŠ¥å‘Šè·å–éœ€è¦åˆ·æ–°çš„è‚¡ç¥¨
    incomplete_stocks = manager.get_incomplete_stocks_from_report(report_file)
    if not incomplete_stocks:
        print("âŒ æ— æ³•è¯»å–æŠ¥å‘Šæˆ–æ²¡æœ‰éœ€è¦åˆ·æ–°çš„è‚¡ç¥¨")
        return False
    
    # åˆå¹¶æ‰€æœ‰éœ€è¦åˆ·æ–°çš„è‚¡ç¥¨
    all_incomplete = []
    for category, stocks in incomplete_stocks.items():
        all_incomplete.extend([stock['symbol'] for stock in stocks])
    
    # å»é‡
    symbols_to_refresh = list(set(all_incomplete))
    
    if not symbols_to_refresh:
        print("âœ… æ‰€æœ‰è‚¡ç¥¨æ•°æ®éƒ½æ˜¯å®Œæ•´çš„ï¼Œæ— éœ€åˆ·æ–°")
        return True
    
    print(f"éœ€è¦åˆ·æ–°çš„è‚¡ç¥¨æ•°é‡: {len(symbols_to_refresh)} åª")
    
    # å¤„ç†æµ‹è¯•æ¨¡å¼å’Œæœ€å¤§è‚¡ç¥¨æ•°é™åˆ¶
    max_stocks = args.max_stocks
    if args.test_mode and not max_stocks:
        max_stocks = 20
    
    if max_stocks:
        print(f"é™åˆ¶å¤„ç†è‚¡ç¥¨æ•°é‡ä¸º: {max_stocks}")
    
    # ç¡®è®¤æ“ä½œ
    if not args.yes:
        response = input(f"\nâš ï¸  å°†åˆ·æ–° {len(symbols_to_refresh)} åªè‚¡ç¥¨çš„æ•°æ®ï¼Œæ˜¯å¦ç»§ç»­ï¼Ÿ(y/n): ").lower().strip()
        if response != 'y':
            print("æ“ä½œå·²å–æ¶ˆ")
            return False
    
    try:
        # 1. å¤‡ä»½æ•°æ®åº“
        print("\n1. å¤‡ä»½æ•°æ®åº“...")
        backup_path = manager.backup_database()
        if not backup_path:
            print("âŒ å¤‡ä»½å¤±è´¥ï¼Œæ“ä½œç»ˆæ­¢")
            return False
        
        # 2. æ¸…é™¤éœ€è¦åˆ·æ–°çš„è‚¡ç¥¨æ•°æ®
        print(f"\n2. æ¸…é™¤ {len(symbols_to_refresh)} åªè‚¡ç¥¨çš„ç°æœ‰æ•°æ®...")
        if not manager.clear_stock_data(symbols_to_refresh):
            print("âŒ æ¸…é™¤æ•°æ®å¤±è´¥ï¼Œæ“ä½œç»ˆæ­¢")
            return False
        
        # 3. é‡æ–°è·å–æ•°æ®
        print("\n3. é‡æ–°è·å–è‚¡ç¥¨æ•°æ®...")
        refresh_results = manager.refresh_stock_data(symbols_to_refresh, days=60, max_stocks=max_stocks)
        
        # 4. æ˜¾ç¤ºç»“æœ
        print("\n" + "=" * 80)
        print("æ™ºèƒ½æ•°æ®åˆ·æ–°å®Œæˆ")
        print("=" * 80)
        print(f"å¤„ç†è‚¡ç¥¨: {refresh_results['total_symbols']} åª")
        print(f"æˆåŠŸè·å–: {refresh_results['success_count']} åª ({refresh_results['success_rate']:.1f}%)")
        print(f"è·å–å¤±è´¥: {refresh_results['failed_count']} åª")
        print(f"æ€»è®°å½•æ•°: {refresh_results['total_records']} æ¡")
        print(f"å¤‡ä»½æ–‡ä»¶: {backup_path}")
        print("=" * 80)
        
        return True
        
    except Exception as e:
        logger.error(f"æ™ºèƒ½åˆ·æ–°è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        print(f"\nâŒ æ“ä½œå¤±è´¥: {e}")
        print(f"ğŸ’¡ å¯ä»¥ä»å¤‡ä»½æ–‡ä»¶æ¢å¤: {backup_path if 'backup_path' in locals() else 'æœªåˆ›å»ºå¤‡ä»½'}")
        return False


def cmd_full_refresh(args, manager):
    """æ‰§è¡Œå…¨é‡åˆ·æ–°å‘½ä»¤"""
    print("\n" + "=" * 60)
    print("å…¨é‡æ•°æ®åˆ·æ–°æ¨¡å¼")
    print("=" * 60)
    print("æ“ä½œæµç¨‹:")
    print("1. å¤‡ä»½å½“å‰æ•°æ®åº“")
    print("2. åˆ é™¤æ‰€æœ‰ç°æœ‰æ—¥çº¿æ•°æ®")
    print("3. é‡æ–°è·å–æ‰€æœ‰è‚¡ç¥¨æ•°æ®")
    
    # å¤„ç†æµ‹è¯•æ¨¡å¼å’Œæœ€å¤§è‚¡ç¥¨æ•°é™åˆ¶
    max_stocks = args.max_stocks
    if args.test_mode and not max_stocks:
        max_stocks = 50
    
    if max_stocks:
        print(f"é™åˆ¶å¤„ç†è‚¡ç¥¨æ•°é‡ä¸º: {max_stocks}")
    
    # ç¡®è®¤æ“ä½œ
    if not args.yes:
        response = input("\nâš ï¸  æ­¤æ“ä½œå°†åˆ é™¤æ‰€æœ‰ç°æœ‰æ—¥çº¿æ•°æ®ï¼Œæ˜¯å¦ç»§ç»­ï¼Ÿ(y/n): ").lower().strip()
        if response != 'y':
            print("æ“ä½œå·²å–æ¶ˆ")
            return False
    
    try:
        # 1. å¤‡ä»½æ•°æ®åº“
        print("\n1. å¤‡ä»½æ•°æ®åº“...")
        backup_path = manager.backup_database()
        if not backup_path:
            print("âŒ å¤‡ä»½å¤±è´¥ï¼Œæ“ä½œç»ˆæ­¢")
            return False
        
        # 2. è·å–è‚¡ç¥¨ä»£ç 
        print("\n2. è·å–è‚¡ç¥¨ä»£ç ...")
        symbols = manager.get_stock_symbols()
        if not symbols:
            print("âŒ æœªæ‰¾åˆ°è‚¡ç¥¨ä»£ç ï¼Œæ“ä½œç»ˆæ­¢")
            return False
        
        # 3. æ¸…é™¤ç°æœ‰æ•°æ®
        print("\n3. æ¸…é™¤æ‰€æœ‰ç°æœ‰æ—¥çº¿æ•°æ®...")
        if not manager.clear_all_daily_data():
            print("âŒ æ¸…é™¤æ•°æ®å¤±è´¥ï¼Œæ“ä½œç»ˆæ­¢")
            return False
        
        # 4. é‡æ–°è·å–æ•°æ®
        print("\n4. é‡æ–°è·å–è‚¡ç¥¨æ•°æ®...")
        refresh_results = manager.refresh_stock_data(symbols, days=60, max_stocks=max_stocks)
        
        # 5. æ˜¾ç¤ºç»“æœ
        print("\n" + "=" * 80)
        print("å…¨é‡æ•°æ®åˆ·æ–°å®Œæˆ")
        print("=" * 80)
        print(f"å¤„ç†è‚¡ç¥¨: {refresh_results['total_symbols']} åª")
        print(f"æˆåŠŸè·å–: {refresh_results['success_count']} åª ({refresh_results['success_rate']:.1f}%)")
        print(f"è·å–å¤±è´¥: {refresh_results['failed_count']} åª")
        print(f"æ€»è®°å½•æ•°: {refresh_results['total_records']} æ¡")
        print(f"å¤‡ä»½æ–‡ä»¶: {backup_path}")
        
        print("\nğŸ’¡ ç°åœ¨å¯ä»¥è¿è¡Œä»¥ä¸‹å‘½ä»¤æ£€æŸ¥æ•°æ®å®Œæ•´æ€§:")
        print("python data_refresh.py check")
        print("=" * 80)
        
        return True
        
    except Exception as e:
        logger.error(f"å…¨é‡åˆ·æ–°è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        print(f"\nâŒ æ“ä½œå¤±è´¥: {e}")
        print(f"ğŸ’¡ å¯ä»¥ä»å¤‡ä»½æ–‡ä»¶æ¢å¤: {backup_path if 'backup_path' in locals() else 'æœªåˆ›å»ºå¤‡ä»½'}")
        return False


def cmd_cleanup(args, manager):
    """æ‰§è¡Œå¤±è´¥è‚¡ç¥¨æ¸…ç†å‘½ä»¤"""
    print("\n" + "=" * 60)
    print("å¤±è´¥è‚¡ç¥¨æ•°æ®å®Œæ•´æ€§æ£€æŸ¥æ¨¡å¼")
    print("=" * 60)
    print("æ“ä½œè¯´æ˜:")
    print("1. æ£€æŸ¥enhanced_batch_progress.jsonä¸­å¤±è´¥è‚¡ç¥¨çš„æ•°æ®å®Œæ•´æ€§")
    print("2. å¦‚æœæ•°æ®å®é™…å®Œæ•´ï¼Œåˆ™ä»å¤±è´¥åˆ—è¡¨ä¸­æ¸…é™¤")
    print("3. æ›´æ–°è¿›åº¦æ–‡ä»¶å¹¶å¤‡ä»½åŸæ–‡ä»¶")
    
    # æ£€æŸ¥è¿›åº¦æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    progress_file = args.progress_file or "data/enhanced_batch_progress.json"
    if not os.path.exists(progress_file):
        print(f"âŒ æœªæ‰¾åˆ°è¿›åº¦æ–‡ä»¶: {progress_file}")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œæ‰¹é‡æ•°æ®è·å–ç¨‹åºç”Ÿæˆè¿›åº¦æ–‡ä»¶")
        return False
    
    # æ˜¾ç¤ºå½“å‰å¤±è´¥è‚¡ç¥¨æ•°é‡
    try:
        with open(progress_file, 'r', encoding='utf-8') as f:
            progress_data = json.load(f)
        failed_count = len(progress_data.get('failed_symbols', []))
        print(f"\nå½“å‰å¤±è´¥è‚¡ç¥¨æ•°é‡: {failed_count} åª")
        
        if failed_count == 0:
            print("âœ… æ²¡æœ‰å¤±è´¥çš„è‚¡ç¥¨éœ€è¦æ£€æŸ¥")
            return True
            
    except Exception as e:
        print(f"âŒ è¯»å–è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}")
        return False
    
    # ç¡®è®¤æ“ä½œ
    if not args.yes:
        response = input(f"\nâš ï¸  å°†æ£€æŸ¥ {failed_count} åªå¤±è´¥è‚¡ç¥¨çš„æ•°æ®å®Œæ•´æ€§ï¼Œæ˜¯å¦ç»§ç»­ï¼Ÿ(y/n): ").lower().strip()
        if response != 'y':
            print("æ“ä½œå·²å–æ¶ˆ")
            return False
    
    try:
        # æ‰§è¡Œå¤±è´¥è‚¡ç¥¨æ¸…ç†
        print("\nå¼€å§‹æ£€æŸ¥å¤±è´¥è‚¡ç¥¨çš„æ•°æ®å®Œæ•´æ€§...")
        cleanup_results = manager.check_and_cleanup_failed_symbols(progress_file)
        
        # æ˜¾ç¤ºç»“æœ
        print("\n" + "=" * 80)
        print("å¤±è´¥è‚¡ç¥¨æ•°æ®å®Œæ•´æ€§æ£€æŸ¥å®Œæˆ")
        print("=" * 80)
        print(f"æ€»å¤±è´¥è‚¡ç¥¨: {cleanup_results.get('total_failed', 0)} åª")
        print(f"å·²æ£€æŸ¥: {cleanup_results.get('checked', 0)} åª")
        print(f"æ•°æ®å®Œæ•´(å·²æ¸…ç†): {cleanup_results.get('cleaned', 0)} åª")
        print(f"ä»ç„¶å¤±è´¥: {cleanup_results.get('still_failed', 0)} åª")
        
        if cleanup_results.get('cleaned', 0) > 0:
            print(f"\nâœ… æˆåŠŸæ¸…ç†äº† {cleanup_results['cleaned']} åªè‚¡ç¥¨")
            print("å·²æ¸…ç†çš„è‚¡ç¥¨ä»£ç :")
            cleaned_symbols = cleanup_results.get('cleaned_symbols', [])
            for i, symbol in enumerate(cleaned_symbols):
                if i % 10 == 0:
                    print()
                print(f"{symbol:>8}", end=" ")
            print()
            
            print(f"\nğŸ’¡ è¿›åº¦æ–‡ä»¶å·²æ›´æ–°ï¼ŒåŸæ–‡ä»¶å·²å¤‡ä»½")
        else:
            print("\nğŸ“ æ²¡æœ‰å‘ç°æ•°æ®å®Œæ•´çš„å¤±è´¥è‚¡ç¥¨")
        
        print("=" * 80)
        return True
        
    except Exception as e:
        logger.error(f"å¤±è´¥è‚¡ç¥¨æ¸…ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        print(f"\nâŒ æ“ä½œå¤±è´¥: {e}")
        return False


def cmd_check(args, manager):
    """æ‰§è¡Œæ•°æ®å®Œæ•´æ€§æ£€æŸ¥å‘½ä»¤"""
    print("\n" + "=" * 60)
    print("æ•°æ®å®Œæ•´æ€§æ£€æŸ¥æ¨¡å¼")
    print("=" * 60)
    print("æ“ä½œè¯´æ˜:")
    print("1. æ£€æŸ¥æ‰€æœ‰è‚¡ç¥¨çš„æ•°æ®å®Œæ•´æ€§ï¼ˆæœ€è¿‘60ä¸ªäº¤æ˜“æ—¥ï¼‰")
    print("2. ç”Ÿæˆè¯¦ç»†çš„å®Œæ•´æ€§æŠ¥å‘Š")
    print("3. æ˜¾ç¤ºç¼ºå¤±æ•°æ®å’Œé‡å¤æ•°æ®çš„ç»Ÿè®¡")
    
    target_days = args.target_days or 60
    output_file = args.output_file or "data/completeness_report.json"
    
    print(f"ç›®æ ‡å¤©æ•°: {target_days}")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
    
    try:
        # æ‰§è¡Œå®Œæ•´æ€§æ£€æŸ¥
        print("\nå¼€å§‹æ£€æŸ¥æ‰€æœ‰è‚¡ç¥¨çš„æ•°æ®å®Œæ•´æ€§...")
        results = manager.check_all_stocks_completeness(target_days=target_days)
        
        if results:
            # ç”ŸæˆæŠ¥å‘Š
            report = manager.generate_completeness_report(results, output_file)
            print(f"\nğŸ“Š è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³: {output_file}")
            
            # æ˜¾ç¤ºå»ºè®®
            summary = results.get('summary', {})
            incomplete_count = summary.get('missing_data', 0) + summary.get('duplicate_data', 0) + summary.get('missing_and_duplicate', 0)
            
            if incomplete_count > 0:
                print(f"\nğŸ’¡ å‘ç° {incomplete_count} åªè‚¡ç¥¨æ•°æ®ä¸å®Œæ•´ï¼Œå»ºè®®è¿è¡Œ:")
                print("   python data_refresh.py smart-refresh")
            else:
                print(f"\nâœ… æ‰€æœ‰è‚¡ç¥¨æ•°æ®å®Œæ•´ï¼Œæ— éœ€åˆ·æ–°")
            
            return True
        else:
            print("âŒ æ£€æŸ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®åº“è¿æ¥")
            return False
            
    except Exception as e:
        logger.error(f"æ•°æ®å®Œæ•´æ€§æ£€æŸ¥è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        print(f"\nâŒ æ“ä½œå¤±è´¥: {e}")
        return False


def create_parser():
    """åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(
        description='æ•°æ®åˆ·æ–°å·¥å…· - æ ¹æ®å®Œæ•´æ€§æ£€æŸ¥æŠ¥å‘Šï¼Œæœ‰é’ˆå¯¹æ€§åœ°åˆ·æ–°ç¼ºå¤±æ•°æ®çš„è‚¡ç¥¨',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # æ™ºèƒ½åˆ·æ–°ï¼ˆæ¨èï¼‰
  python data_refresh.py smart-refresh
  python data_refresh.py smart-refresh --test-mode --yes
  
  # å…¨é‡åˆ·æ–°
  python data_refresh.py full-refresh --max-stocks 100
  
  # æ¸…ç†å¤±è´¥è‚¡ç¥¨åˆ—è¡¨
  python data_refresh.py cleanup --yes
  
  # æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
  python data_refresh.py check --target-days 60
        """
    )
    
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # smart-refresh å­å‘½ä»¤
    smart_parser = subparsers.add_parser(
        'smart-refresh',
        help='åŸºäºæŠ¥å‘Šçš„æ™ºèƒ½åˆ·æ–°ï¼ˆæ¨èï¼‰',
        description='æ ¹æ®å®Œæ•´æ€§æŠ¥å‘Šï¼Œåªåˆ·æ–°æœ‰é—®é¢˜çš„è‚¡ç¥¨æ•°æ®'
    )
    smart_parser.add_argument('--test-mode', action='store_true',
                             help='æµ‹è¯•æ¨¡å¼ï¼ˆé™åˆ¶å¤„ç†è‚¡ç¥¨æ•°é‡ï¼‰')
    smart_parser.add_argument('--max-stocks', type=int,
                             help='æœ€å¤§å¤„ç†è‚¡ç¥¨æ•°é‡')
    smart_parser.add_argument('--report-file',
                             help='å®Œæ•´æ€§æŠ¥å‘Šæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: data/completeness_report.jsonï¼‰')
    smart_parser.add_argument('--yes', action='store_true',
                             help='è·³è¿‡ç¡®è®¤æç¤ºï¼Œç›´æ¥æ‰§è¡Œ')
    
    # full-refresh å­å‘½ä»¤
    full_parser = subparsers.add_parser(
        'full-refresh',
        help='å…¨é‡æ•°æ®åˆ·æ–°',
        description='åˆ é™¤æ‰€æœ‰ç°æœ‰æ•°æ®ï¼Œé‡æ–°è·å–æ‰€æœ‰è‚¡ç¥¨æ•°æ®'
    )
    full_parser.add_argument('--test-mode', action='store_true',
                            help='æµ‹è¯•æ¨¡å¼ï¼ˆé™åˆ¶å¤„ç†è‚¡ç¥¨æ•°é‡ï¼‰')
    full_parser.add_argument('--max-stocks', type=int,
                            help='æœ€å¤§å¤„ç†è‚¡ç¥¨æ•°é‡')
    full_parser.add_argument('--yes', action='store_true',
                            help='è·³è¿‡ç¡®è®¤æç¤ºï¼Œç›´æ¥æ‰§è¡Œ')
    
    # cleanup å­å‘½ä»¤
    cleanup_parser = subparsers.add_parser(
        'cleanup',
        help='æ£€æŸ¥å¹¶æ¸…ç†å¤±è´¥è‚¡ç¥¨åˆ—è¡¨',
        description='æ£€æŸ¥å¤±è´¥è‚¡ç¥¨çš„æ•°æ®å®Œæ•´æ€§ï¼Œå¦‚æœå®Œæ•´åˆ™ä»å¤±è´¥åˆ—è¡¨ä¸­æ¸…é™¤'
    )
    cleanup_parser.add_argument('--progress-file',
                               help='æ‰¹å¤„ç†è¿›åº¦æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: data/enhanced_batch_progress.jsonï¼‰')
    cleanup_parser.add_argument('--yes', action='store_true',
                               help='è·³è¿‡ç¡®è®¤æç¤ºï¼Œç›´æ¥æ‰§è¡Œ')
    
    # check å­å‘½ä»¤
    check_parser = subparsers.add_parser(
        'check',
        help='æ•°æ®å®Œæ•´æ€§æ£€æŸ¥',
        description='æ£€æŸ¥æ‰€æœ‰è‚¡ç¥¨çš„æ•°æ®å®Œæ•´æ€§å¹¶ç”ŸæˆæŠ¥å‘Š'
    )
    check_parser.add_argument('--target-days', type=int, default=60,
                             help='æ£€æŸ¥çš„ç›®æ ‡å¤©æ•°ï¼ˆé»˜è®¤: 60ï¼‰')
    check_parser.add_argument('--output-file',
                             help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: data/completeness_report.jsonï¼‰')
    
    return parser


def main():
    """ä¸»å‡½æ•° - å¤„ç†å‘½ä»¤è¡Œå‚æ•°å¹¶æ‰§è¡Œç›¸åº”æ“ä½œ"""
    parser = create_parser()
    args = parser.parse_args()
    
    # å¦‚æœæ²¡æœ‰æä¾›å‘½ä»¤ï¼Œæ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
    if not args.command:
        parser.print_help()
        return
    
    print("=" * 80)
    print("æ™ºèƒ½æ•°æ®åˆ·æ–°å·¥å…·")
    print("æ ¹æ®å®Œæ•´æ€§æ£€æŸ¥æŠ¥å‘Šï¼Œæœ‰é’ˆå¯¹æ€§åœ°åˆ·æ–°ç¼ºå¤±æ•°æ®çš„è‚¡ç¥¨")
    print("=" * 80)
    
    # åˆ›å»ºæ•°æ®åˆ·æ–°ç®¡ç†å™¨
    manager = DataRefreshManager()
    
    # æ ¹æ®å‘½ä»¤æ‰§è¡Œç›¸åº”æ“ä½œ
    success = False
    try:
        if args.command == 'smart-refresh':
            success = cmd_smart_refresh(args, manager)
        elif args.command == 'full-refresh':
            success = cmd_full_refresh(args, manager)
        elif args.command == 'cleanup':
            success = cmd_cleanup(args, manager)
        elif args.command == 'check':
            success = cmd_check(args, manager)
        else:
            print(f"âŒ æœªçŸ¥å‘½ä»¤: {args.command}")
            parser.print_help()
            return
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æ“ä½œè¢«ç”¨æˆ·ä¸­æ–­")
        return
    except Exception as e:
        logger.error(f"æ‰§è¡Œå‘½ä»¤ {args.command} æ—¶å‡ºé”™: {e}")
        print(f"\nâŒ æ‰§è¡Œå¤±è´¥: {e}")
        return
    
    # æ ¹æ®æ‰§è¡Œç»“æœè®¾ç½®é€€å‡ºç 
    if success:
        print(f"\nâœ… å‘½ä»¤ '{args.command}' æ‰§è¡ŒæˆåŠŸ")
        sys.exit(0)
    else:
        print(f"\nâŒ å‘½ä»¤ '{args.command}' æ‰§è¡Œå¤±è´¥")
        sys.exit(1)


if __name__ == "__main__":
    main()